# humana_chatbot

## Running the chatbot

1. Clone the repository and create a conda environment based on the environment.yml file
2. streamlit run app.py

The app should launch in a browser tab. It may take a moment for the pdf to load, then you can ask questions. Note that the app uses gpt2 as a foundation model so that it can run on a cpu. High quality responses should not be expected; this app is intended only as a prototype.

## Viewing the evaluation

The file eval_report.html contains information about the chatbot's performance across five dimensions. For further information about how the evaluation was conducted, and for parallel evaluations of gpt2-based and llama-based chatbots using the same RAG system, view the contents of the evaluation directory.

## Reflection

### Suggestions for improvement
The chatbot and evaluation system presented here are only a prototype and numerous improvements are possible. 

#### Low-hanging fruit
- **GPU access + better models**: The foundation model supporting this chatbot is gpt2, which has 137M parameters. The chatbot's answers are low quality by most metrics presented in the evaluation and based on visual inspection of individual examples, the model responses are often incoherent. I additionally tested the same chatbot system using Llama-3.2-1B, which has 1.24B parameters. This chatbot performed considerably better, suggesting that significant performance gains can be achieved with larger models. However, for this prototype I was limited to what I was able to run on a CPU. Even just a 1-billion parameter model, which is quite small on the scale of LLMs, resulted in response lags of up to 30 seconds. If this chatbot were to be converted into a productionizable project, the first step would be to establish GPU access and identify a better foundation model.
- **Short term memory for multiturn interactions**: The current chatbot implementation allows for multiturn interactions in the streamlit UI, but each model completion is based on only the most recent user turn and the passages retrieved based on that prompt. Implementing a memory mechanism whereby previous conversational turns are incorporated into each prompt is straightforward using langchain, but was not pursued for this prototype.
- **Better OCR**: The pdf containing the Slamon et al. (1987) article was parsed using the pdfplumber package in order to create a text representation of the document that could then be chunked and converted into embeddings. While this strategy worked well enough for a prototype, visual inspection of the text file generated by pdfplumber (see knowledge_base/parsed_documents/slamon_etal.txt) suggests that there's room for improvement. The pdfplumber package does not natively handle multi-column texts (the work-around implemented here was to split each page vertically along the midline and concatenate the parsed half-pages) and appears to struggle to identify word boundaries consistently. Using an LLM-based OCR approach (one candidate is https://huggingface.co/reducto/RolmOCR) could potentially yield significant improvements.

#### Long term goals
- **Expanded, expert-authored benchmarking dataset**: The chatbot presented here was evaluated using a benchmarking dataset of only 30 prompts, which were all synthetically generated by a very small LLM with no medical domain expertise (Qwen3-0.6B). Thus, there is reason for caution in interpreting the evaluation results. Pivoting to human-authoring or a combination of human-authoring and synthetic generation could yield a more reliable benchmarking dataset. Ideally, humans with domain expertise and a model that has been finetuned on medical or biology data would be used. In addition, in order to obtain the precision necessary to make comparisons between candidate chatbots, a much larger benchmarking dataset is needed. A power analysis can be conducted to determine the necessary volume.
- **Expert-validated LLM-as-a-judge and validated metrics**: The chatbot presented here was evaluated based on LLM judgments of four dimensions (accuracy, helpfulness, groundedness, retrieval relevance) using a very small LLM with no medical domain expertise (Qwen3-0.6B), as well as response time. In order for these LLM judgments to be trustworthy, we would need to validate them using a ground truth sample, such as annotations of the same four dimensions by human annotators with medical expertise. A systematic investigation of the correlation between human judgments and LLM judgments would allow us to use the LLM for a larger-volume evaluation with confidence. Through prompt engineering, this correlation could be improved. Additionally, validation of the metrics themselves using human annotators would support greater confidence in the evaluation.
- **Medical LLM foundation model**: As discussed above, larger models would likely yield big gains in performance. If, however, even with a state of the art general purpose model performance still suffers, pivoting to a medical LLM such as MMed-Llama would likely lead to further improvements.
- **Medical LLM sentence embedding model**: The all-MiniLM-L6-v2 sentence transformer was used to generate embeddings for the text chunks and user queries in order to enable the RAG system. Retrieval relevance was generally quite high (4.67 on a 1-5 scale) based on the evaluation presented here, there is room for improvement. Some improvement could likely be obtained by improving the OCR model, as discussed above (since retrieving relevant chunks is made much more difficult when the chunks themselves are incoherent), but further improvement could be obtained by using a medically-informed sentence embedding model.
- **Incorporating ClinVec**: ClinVec embeddings could be used to improve either the foundation model or the sentence embedding model. See the "discussion of ClinVec" section below for further details.

### Overview of the chatbot

This chatbot was developed to answer questions about Slamon et al. (1987) using a RAG system. In order to prepare the pdf for ingestion into the RAG system, the following steps were taken: The pdf of Slamon et al. (1987) was parsed and segmented into chunks for retrieval. Embeddings for those chunks were created using the all-MiniLM-L6-v2 sentence transformer, and an index was created using FAISS. When the chatbot is prompted, the user prompt is then used to retrieve relevant chunks from the knowledge base, and those chunks are inserted into a prompt template, along with the user prompt. Because the entire system is designed to run on a cpu, only very small models can be used. GPT-2 was selected because it allowed response speeds of approximately 8 seconds, making it a functional prototype. Note, however, that the accuracy of GPT-2's responses is extremely low and the outputs are often incoherent. 

### Overview of the evaluation procedure

The chatbot was evaluated using a small synthetic dataset of 30 questions. The questions were generated using Qwen/Qwen3-0.6B. Again, in order to run on a CPU, small models had to be selected. A model from a separate family was selected in order to prevent a biased evaluation.

The model was prompted three times, each time to generate 10 questions that a potential user might ask about the target article (Slamon et al., 1987). Each of the prompts instructed the model to simulate a different hypothetical user: general readers, domain experts (such as those with an advanced degree in medicine, biology, or genetics), and breast cancer patients. The 30 questions generated this way were then used to prompt the chatbot (once per prompt), and responses were recorded and evaluated.

Chatbot responses were evaluated based on five dimensions: response time, retrieval relevance, response groundedness, response helpfulness, and response accuracy. Dimensions which were evaluated using an LLM as a judge used Qwen/Qwen3-0.6B.

- Response Time: This dimension was evaluated automatically by recording the timestamp immediately before and after generating the chatbotâ€™s response to the query.
- Retrieval Relevance: This dimension was evaluated using an LLM as a judge. The LLM was prompted to provide a numerical relevance rating on a scale from 1 to 5 and was provided with both the query and the retrieved passages. Higher scores indicate that the retrieved passages were more relevant to the query, defined as containing the answer to the query.
- Response Groundedness: This dimension was evaluated using an LLM as a judge. The LLM was prompted to provide a numerical groundedness rating on a scale from 1 to 5 and was provided with both the chatbot response and the retrieved passages. Higher scores indicate that the chatbot response was more grounded in the retrieved passages; lower scores indicate the presence of hallucinations.
- Response Helpfulness: This dimension was evaluated using an LLM as a judge. The LLM was prompted to provide a numerical helpfulness rating on a scale from 1 to 5 and was provided with both the query and the chatbot response. Higher scores indicate that the response was more helpful to the hypothetical user who submitted the query.
- Response Accuracy: This dimension was evaluated using an LLM as a judge. The LLM was prompted to provide a numerical accuracy rating on a scale from 1 to 5 and was provided with the query, the retrieved passages, and the chatbot response. Higher scores indicate that the response was a more accurate response to the query, given the retrieved passages.

The evaluation procedure was run twice: once with GPT-2 as the foundation model supporting the chatbot and once with Llama-3.2-1B. The comparison of these two evaluations reveals that substantial gains can be made in response quality by using a bigger model, though this of course comes with timing costs.

### Discussion of ClinVec

1. What are the 3 takeaways for this research?
     a. ClinGraph is a knowledge graph representing "diagnosis codes (ICD9CM, ICD10CM, phecodes), laboratory tests (LOINC), medications (RxNorm, ATC), procedures (CPT), and other controlled clinical vocabularies (SNOMED CT, UMLS)". ClinGraph was used to create ClinVec, a set of embeddings of clinical terms.
     b. ClinVec embedding similarity scores closely track expert judgments of the similarity of terms. In addition, the organization of the ClinVec space captures natural groupings of the concepts encoded by ClinVec - for example, organ systems. These facts, among other demonstrations provided by the authors, suggest that the ClinVec embedding space successfully captures the types of conceptual similarity that are most relevant in medical applications.
     c. ClinVec embeddings can be used to fine-tune Large Language Models, resulting in improved accuracy scores in a question-answering task based on US Medical Licensing Exams as well as a dataset of medical questions collected from healthcare settings across 16 African countries. This success suggests that ClinVec embeddings may have broad utility in adapting LLMs to medical usecases.
3. How could this research be important for Humana?
     ClinVec embeddings could be used for a wide range of LLM applications in medical settings. For example, one potential usecase could be an AI Assistant for medical providers to more efficiently review an individual patient's medical records including previous clinical notes. While some components of medical records may be structured databases, other components will include text written by providers about the patients presenting symptoms and diagnosis. In some cases, transcripts of doctor-patient interactions could also be available. Any freeform text will present difficulties for efficiently pulling up relevant information, and this situation is well suited to AI support in the form of a RAG chatbot that could answer the provider's questions about the patient's medical history. However, such a chatbot would only be useful if it is accurate. Based on this research, there is reason to suspect that ClinVec embeddings could significantly improve a system like this. ClinVec could be useful in many other applications as well. For example, a call center worker discussing a patient's coverage under their humana health plan could be aided by an AI chatbot that extracts relevant information from policy documents. In this situation as well, accuracy is important and fine-tuning the underlying LLM with ClinVec could support better accuracy.
4. How can you include this research into your Q/A chatbot?
     There are two broad approaches for including ClinVec in a RAG QA chatbot like the one developed here. The first is to use ClinVec embeddings to improve upon the sentence embedding model, in order to boost retrieval relevance. The second is to use ClinVec embeddings to improve upon the question-answering model, in order to boost response accuracy. I did not pursue either of these strategies in the implementation delivered here, but they could be valuable avenues to explore after the 'low hanging fruit' discussed above.
